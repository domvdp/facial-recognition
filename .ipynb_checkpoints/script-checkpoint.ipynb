{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recognition Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependicies\n",
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Import tensorflow dependicies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "ANC_PATH = os.path.join('..', 'data', 'anchor')\n",
    "POS_PATH = os.path.join('..', 'data', 'positive')\n",
    "NEG_PATH = os.path.join('..', 'data', ' negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cropping_boundaries(frame_dim, target_dim) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the pixel boundaries (width, height) for cropping the webcam frame.\n",
    "    \"\"\"\n",
    "    frame_dim_centre = int(frame_dim / 2)\n",
    "    half_target_dim = int(target_dim / 2)\n",
    "    \n",
    "    starting_pixel = frame_dim_centre - half_target_dim\n",
    "    ending_pixel = frame_dim_centre + half_target_dim\n",
    "    \n",
    "    return starting_pixel, ending_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allow user to take pictures of their face (using laptop webcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webcam(width_pixels, height_pixels, key):\n",
    "    # Establish connection to webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        val, frame = cap.read()\n",
    "\n",
    "        # Identify width and height of webcam frame\n",
    "        frame_width = frame.shape[0]\n",
    "        frame_height = frame.shape[1]\n",
    "\n",
    "        # Calculate boundaries for central cropping\n",
    "        start_width, end_width = calc_cropping_boundaries(frame_width, width_pixels)\n",
    "        start_height, end_height = calc_cropping_boundaries(frame_height, height_pixels)\n",
    "\n",
    "        # Crop frame so that captured images are same size as labelled images (dimensions 250 by 250 pixels)\n",
    "        cropped_frame = frame[start_width:end_width, start_height:end_height, :]\n",
    "        \n",
    "        if key == 'a':\n",
    "            # Capture anchor image\n",
    "            if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "                # Create unique file path for anchor image\n",
    "                img_path = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "\n",
    "                # Write anchor image to unique file path\n",
    "                cv2.imwrite(img_path, cropped_frame)\n",
    "                \n",
    "        if key == 'p':\n",
    "            # Capture positive image\n",
    "            if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "                # Create unique file path for positive image\n",
    "                img_path = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "\n",
    "                # Write anchor image to unique file path\n",
    "                cv2.imwrite(img_path, cropped_frame)\n",
    "\n",
    "        # Show image back to screen\n",
    "        cv2.imshow('Image cap', cropped_frame)\n",
    "\n",
    "        # Wait for a key press, and break loop if 'q' key is held\n",
    "        if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam\n",
    "    cap.release()\n",
    "\n",
    "    # Close image show frame\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired size of image [width, height] so that frame can be cropped\n",
    "width_pixels, height_pixels = [250, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Anchor images with webcam\n",
    "webcam(width_pixels, height_pixels, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Positive images with webcam\n",
    "webcam(width_pixels, height_pixels, 'p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull image paths in a streaming fashion\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(400)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(400)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the image paths from within the tensors\n",
    "# dir_test = anchor.as_numpy_iterator()\n",
    "\n",
    "# # Iterate through the image paths one at a time\n",
    "# dir_test.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file_path):\n",
    "    # Read image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Decode image\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess(file_path):\n",
    "    \n",
    "    img = read_img(file_path)\n",
    "    \n",
    "    # Resize the image to be 105 x 105 x 3 (chose 105 as per Siamese Neural Network research paper)\n",
    "    img = tf.image.resize(img, (105, 105))\n",
    "    \n",
    "    # Scale the pixels to be values between 0 and 1\n",
    "    img = img / 255\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader pipeline\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data) * 0.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data) * 0.7))\n",
    "test_data = data.take(round(len(data) * 0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(): \n",
    "    input_layer = Input(shape=(105,105,3), name='input_image')\n",
    "    \n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(input_layer)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[input_layer], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 105, 105, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 96, 96, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 42, 42, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 21, 21, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 18, 18, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding = make_embedding()\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build distance layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    # Calculate similarity (distance)\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    # Input anchor image \n",
    "    input_image = Input(name='input_img', shape=(105,105,3))\n",
    "    \n",
    "    # Input validation image\n",
    "    validation_image = Input(name='validation_img', shape=(105,105,3))\n",
    "    \n",
    "    # Combine Siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_siamese_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
